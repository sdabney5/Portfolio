{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159b31c2",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184c075",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39aeab1",
   "metadata": {},
   "source": [
    "\n",
    "In Logistic Regression, we fit a curve to data that represents the probability of a binary outcome. We then use this curve to predict the probability of the outcome for new data points.\n",
    "\n",
    "The objective of this project is to code a Logistic Regression function from scratch without using imported python libraries. It follows my completion of the Linear Regression from scratch project. This project still involves finding the best coefficient values for the hypothesis function, but in the case the hypothesis function is a sigmoid function. We use gradient descent to optimize the coefficients, as in Linear Regression.\n",
    "\n",
    "Coding Logistic Regression from scratch may be more complex than Linear Regression. However, by building the algorithm from the ground up, I hope to gain a better understanding of the underlying math and logic involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4768059",
   "metadata": {},
   "source": [
    "## Background: Probability and Odds in Logistic Regression\n",
    "In logistic regression, we are interested in estimating the probability of a binary outcome, such as whether a customer will buy a product or not, based on one or more predictor variables. In order to understand logistic regression, it's important to have a basic understanding of probability and odds.\n",
    "\n",
    "#### Probability\n",
    "Probability is a measure of the likelihood of an event occurring. It is defined as the number of favorable outcomes divided by the total number of possible outcomes. For example, the probability of rolling a 1 or 2 on a fair six-sided die is $\\frac{2}{6}$, or $\\frac{1}{3}$, or approximately 0.333.\n",
    "\n",
    "#### Odds\n",
    "Odds are another way to express the likelihood of an event occurring. Odds are defined as the probability of the event occurring divided by the probability of the event not occurring. Mathematically, this can be expressed as:\n",
    "\n",
    "$$Odds = \\frac{P(event)}{1-P(event)}$$\n",
    "\n",
    "For example, the odds of rolling a 1 or 2 on a fair six-sided die can be calculated as follows:\n",
    "\n",
    "$$Odds(1\\ or\\ 2) = \\frac{P(1\\ or\\ 2)}{P(not\\ 1\\ or\\ 2)} = \\frac{2/6}{4/6} = \\frac{1}{2}$$\n",
    "\n",
    "#### Odds Ratio\n",
    "In logistic regression, we are interested in the odds ratio, which is the ratio of two odds. For example, the odds ratio of buying a product between two different age groups might be the odds of buying the product for the older group divided by the odds of buying the product for the younger group.\n",
    "\n",
    "#### Logistic Function\n",
    "The logistic regression model estimates the probability of an event occurring based on one or more predictor variables. This is done by using a logistic or sigmoid function, which maps the predictor variables to a probability between 0 and 1. The logistic function is defined as:\n",
    "\n",
    "$$logit(p) = ln\\left(\\frac{p}{1-p}\\right)$$\n",
    "\n",
    "where $p$ is the probability of the event occurring. The logistic function is useful because it is undefined at $p=0$ and $p=1$, which are the extreme values of the probability.\n",
    "\n",
    "#### Inverse Logit\n",
    "The inverse logit function, also known as the logistic function, is used to convert the output of the logistic regression model back into a probability. It is defined as:\n",
    "\n",
    "$$p = \\frac{e^a}{1+e^a}$$\n",
    "\n",
    "where $a$ is the linear combination of the predictor variables and their coefficients, also known as the log odds or logit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adb17a",
   "metadata": {},
   "source": [
    "## Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186de87",
   "metadata": {},
   "source": [
    "\n",
    "In Logistic Regression, the hypothesis function uses the sigmoid function to model the probability of a binary outcome. The sigmoid function is defined as:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "where $z$ is the linear combination of the input features and weights, represented as:\n",
    "\n",
    "$$ z = \\theta_{0} x_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ... + \\theta_{n} x_{n} $$\n",
    "\n",
    "Here, $\\theta_{0}$ corresponds to the bias term, and $x_{0}$ is set to 1 for all input examples. The hypothesis function is then defined as:\n",
    "\n",
    "$$ h_{\\theta}(x) = g(z) = \\frac{1}{1 + e^{-\\vec{\\theta^{\\top}} \\vec{x}}} $$\n",
    "\n",
    "The cross entropy (log-likelihood) Loss Function:\n",
    "For Logistic Regression, we use the log-likelihood or cross entropy loss function to optimize the parameters of the model, rather than the MSE:\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\right] $$\n",
    "\n",
    "* $m$ is the number of training examples,  \n",
    "* $y^{(i)}$ is the true label (0 or 1) for the $i$th example  \n",
    "* $h_{\\theta}(x^{(i)})$ is the predicted probability that the $i$th example belongs to the positive class (i.e., has label 1).\n",
    "\n",
    "## The Gradient Descent Algorithm:\n",
    "To minimize the loss function and find the optimal values of the weights $\\theta$, we again use gradient descent. The gradient of the loss function with respect to $\\theta_{j}$ is:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x_{j}^{(i)} $$\n",
    "\n",
    "We then update each $\\theta$ using the learning rate  $\\alpha$:\n",
    "\n",
    "$$ \\theta_{j} = \\theta_{j} - \\alpha \\frac{\\partial J}{\\partial \\theta_{j}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3095756",
   "metadata": {},
   "source": [
    "## The Cross Entropy Loss Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dc2a1",
   "metadata": {},
   "source": [
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf52e75",
   "metadata": {},
   "source": [
    "Note: Technically, we will not need to use this loss function in the code, since we only need the gradient of the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339133a",
   "metadata": {},
   "source": [
    "#### Gradient of Cross Entropy Loss Function\n",
    "We end up with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd0783",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a980e",
   "metadata": {},
   "source": [
    "Note that this ends up looking the same as when we calculate the gradient of the MSE.  However, the difference between the two loss functions is in the way that the predicted output $h_\\theta(x^{(i)})$ is computed. For logistic regression, the predicted output is computed using the sigmoid function. For linear regression, it is just computed as a linear combination of the input features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f1664",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112842ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of Euler's number. \n",
    "# I am doing this instead of math.exp simply because this project does not use imported modules or libraries\n",
    "e = 2.71828\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    #initialize hyperparameters\n",
    "    def __init__(self, alpha=0.001, num_iterations=1000, threshold = 0.5):\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations\n",
    "        self.theta = None\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Add a column of 1's to X to represent the intercept\n",
    "        X = X.insert(0, \"ONES\", 1) \n",
    "        \n",
    "        # Rows (m) and columns (n)\n",
    "        m , n = X.shape\n",
    "        \n",
    "        # Initialize a list of 0's length \"n\" for first theta values\n",
    "        self.theta = [0]*n\n",
    "        \n",
    "        # Hypothesis function\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Create a 0's list of predicted values for y_hat\n",
    "            y_hat = [0]*m \n",
    "            for i in range(m):\n",
    "                for j in range(n):\n",
    "                    y_hat[i] += (X[i][j] * self.theta[j])  #this is just the hypothesis function from linear regression.\n",
    "                \n",
    "                # For Logistic Regression, we need to apply the sigmoid function to each predicted value                \n",
    "                y_hat[i] = 1 / (1 + e**(-y_hat[i])) \n",
    "                        \n",
    "            # Gradient calculation of cross-entropy (J(theta)) w.r.t. theta[j]\n",
    "            dJ_dtheta = [0]*n\n",
    "            \n",
    "            for j in range(n):\n",
    "                for i in range(m):\n",
    "                    dJ_dtheta[j] += 1/m * ((y_hat[i] - y[i]) * X[i][j]) # This gives the partial derivative of the cross-entropy w.r.t theta[j]\n",
    "             \n",
    "            # Update theta values. Previous theta value minus the corresponding gradient value times the learning rate\n",
    "            for j in range(n):\n",
    "                self.theta[j] -=  self.alpha * dJ_dtheta[j]\n",
    "            \n",
    "    def predict_prob(self, X):\n",
    "        # Add a column of 1's to X to represent the intercept\n",
    "        X = X.insert(0, \"ONES\", 1) \n",
    "        \n",
    "        # Rows (m) and columns (n)\n",
    "        m , n = X.shape \n",
    "        \n",
    "        # Create a list of zeros for probability predictions\n",
    "        y_predicted_prob = [0]*m\n",
    "        \n",
    "        # Compute predictions\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                y_predicted_prob[i] += X[i][j] * self.theta[j]\n",
    "                \n",
    "                # Apply sigmoid function to the predicted values\n",
    "                y_predicted_prob[i] = 1 / (1 + e**(-y_predicted_prob[i]))\n",
    "\n",
    "        return y_predicted_prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Add a column of 1's to X to represent the intercept\n",
    "        X = X.insert(0, \"ONES\", 1) \n",
    "        \n",
    "        # Rows (m) and columns (n)\n",
    "        m , n = X.shape \n",
    "        \n",
    "        # Create a list of zeros for class predictions and probability predictions\n",
    "        y_prob_pred = [0]*m\n",
    "        y_pred  = [0]*m\n",
    "        \n",
    "        # Compute class predictions according to the threshold\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                y_prob_pred[i] += X[i][j] * self.theta[j]\n",
    "                \n",
    "                # Apply sigmoid function to the predicted values\n",
    "                y_prob_pred[i] = 1 / (1 + e**(-y_prob_pred[i]))\n",
    "                \n",
    "            if y_prob_pred[i] >= self.threshold:\n",
    "                y_pred[i] = 1\n",
    "        \n",
    "\n",
    "        return y_pred\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bef19",
   "metadata": {},
   "source": [
    "## Insights and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e4c2b",
   "metadata": {},
   "source": [
    "Conceptually, I find logisitic regression more difficult and less intuitive than Linear Regression. However, I feel I learned quite a bit by building this. \n",
    "\n",
    "Like my Linear Regression project, there are many ways to improve. For example, Numpy would make the code more efficient. However, I chose not to use Numpy to ensure that I understood every step of the process. Avoiding numpy also demonstrates that this project was my own work, since all online tutorials rely on Numpy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
